# PostgreSQL Datakits

A comprehensive guide to building datakits that extract data from PostgreSQL databases, including connection management, query optimization, and PostgreSQL-specific features.

## ðŸŽ¯ PostgreSQL-Specific Features

PostgreSQL datakits leverage unique PostgreSQL capabilities:
- **COPY protocol** for high-speed bulk extraction
- **Logical replication** for real-time CDC
- **Partitioned tables** for efficient large table handling
- **JSONB support** for semi-structured data
- **Array types** for complex data structures
- **Custom types and domains**

## ðŸ“š Quick Start

### Basic PostgreSQL Datakit

```python
# datakit_postgres/extractor.py
import pandas as pd
import psycopg2
from psycopg2 import pool
from sqlalchemy import create_engine, inspect
from contextlib import contextmanager
import os
import logging
from typing import Optional, Dict, Any, Generator
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PostgreSQLExtractor:
    """Extract data from PostgreSQL to Bronze layer."""

    def __init__(self):
        self.source_config = self._parse_connection_string(
            os.getenv('SOURCE_DATABASE_URL')
        )
        self.target_url = os.getenv('WAREHOUSE_DATABASE_URL')
        self.connection_pool = self._create_pool()

    def _parse_connection_string(self, url: str) -> Dict[str, Any]:
        """Parse PostgreSQL connection string."""
        from urllib.parse import urlparse
        parsed = urlparse(url)

        return {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'database': parsed.path.lstrip('/'),
            'user': parsed.username,
            'password': parsed.password,
            'sslmode': 'prefer',
            'connect_timeout': 10,
            'application_name': 'datakit-postgres-extractor'
        }

    def _create_pool(self) -> psycopg2.pool.ThreadedConnectionPool:
        """Create connection pool for efficient connection management."""
        return psycopg2.pool.ThreadedConnectionPool(
            minconn=1,
            maxconn=10,
            **self.source_config
        )

    @contextmanager
    def get_connection(self) -> Generator[psycopg2.extensions.connection, None, None]:
        """Get connection from pool."""
        conn = self.connection_pool.getconn()
        try:
            yield conn
        finally:
            self.connection_pool.putconn(conn)

    def extract_table(
        self,
        table_name: str,
        schema: str = 'public',
        where_clause: Optional[str] = None,
        columns: Optional[list] = None
    ) -> int:
        """Extract a table from PostgreSQL to Bronze."""
        logger.info(f"Starting extraction for {schema}.{table_name}")

        # Build query
        query = self._build_extraction_query(
            schema, table_name, columns, where_clause
        )

        # Extract data
        df = self._extract_with_chunking(query)

        # Add Bronze metadata
        df['_bronze_loaded_at'] = datetime.now()
        df['_bronze_source'] = f"postgresql://{self.source_config['host']}/{self.source_config['database']}"
        df['_bronze_schema'] = schema
        df['_bronze_table'] = table_name

        # Write to Bronze
        rows_written = self._write_to_bronze(df, table_name)

        logger.info(f"Successfully extracted {rows_written} rows from {schema}.{table_name}")
        return rows_written

    def _build_extraction_query(
        self,
        schema: str,
        table_name: str,
        columns: Optional[list] = None,
        where_clause: Optional[str] = None
    ) -> str:
        """Build optimized extraction query."""
        # Select columns
        if columns:
            column_list = ', '.join([f'"{col}"' for col in columns])
        else:
            column_list = '*'

        # Base query
        query = f'SELECT {column_list} FROM "{schema}"."{table_name}"'

        # Add WHERE clause
        if where_clause:
            query += f' WHERE {where_clause}'

        return query

    def _extract_with_chunking(self, query: str, chunk_size: int = 10000) -> pd.DataFrame:
        """Extract data in chunks to manage memory."""
        chunks = []

        # Use server-side cursor for memory efficiency
        with self.get_connection() as conn:
            # Create server-side cursor
            cursor_name = f'extraction_cursor_{datetime.now().timestamp()}'
            with conn.cursor(name=cursor_name) as cursor:
                cursor.itersize = chunk_size
                cursor.execute(query)

                # Fetch column names
                columns = [desc[0] for desc in cursor.description]

                # Fetch data in chunks
                while True:
                    rows = cursor.fetchmany(chunk_size)
                    if not rows:
                        break

                    chunk_df = pd.DataFrame(rows, columns=columns)
                    chunks.append(chunk_df)

                    logger.debug(f"Processed chunk with {len(chunk_df)} rows")

        # Combine chunks
        if chunks:
            return pd.concat(chunks, ignore_index=True)
        else:
            return pd.DataFrame()

    def _write_to_bronze(self, df: pd.DataFrame, table_name: str) -> int:
        """Write DataFrame to Bronze layer."""
        if df.empty:
            logger.warning(f"No data to write for {table_name}")
            return 0

        target_engine = create_engine(self.target_url)

        # Write to Bronze schema
        df.to_sql(
            name=f'br_{table_name}',
            con=target_engine,
            schema='bronze',
            if_exists='replace',
            index=False,
            method='multi',  # Use multi-row insert
            chunksize=10000
        )

        return len(df)
```

## ðŸš€ Advanced Features

### **1. COPY Protocol for High-Speed Extraction**

```python
import io
import csv

class FastPostgreSQLExtractor(PostgreSQLExtractor):
    """Use COPY protocol for fast extraction."""

    def extract_table_fast(self, schema: str, table_name: str) -> int:
        """Extract using COPY TO for maximum speed."""
        logger.info(f"Fast extraction using COPY for {schema}.{table_name}")

        with self.get_connection() as conn:
            # Create a StringIO buffer
            buffer = io.StringIO()

            # Use COPY TO to export data
            with conn.cursor() as cursor:
                copy_sql = f"""
                    COPY (
                        SELECT *,
                            CURRENT_TIMESTAMP as _bronze_loaded_at,
                            '{self.source_config['database']}' as _bronze_source
                        FROM "{schema}"."{table_name}"
                    ) TO STDOUT WITH (
                        FORMAT CSV,
                        HEADER TRUE,
                        DELIMITER ',',
                        NULL '\\N',
                        QUOTE '"',
                        ESCAPE '\\\\'
                    )
                """
                cursor.copy_expert(copy_sql, buffer)

            # Read buffer into DataFrame
            buffer.seek(0)
            df = pd.read_csv(
                buffer,
                na_values=['\\N'],
                parse_dates=['_bronze_loaded_at']
            )

            # Write to Bronze using COPY FROM for speed
            return self._write_with_copy(df, f'br_{table_name}')

    def _write_with_copy(self, df: pd.DataFrame, target_table: str) -> int:
        """Write using COPY FROM for maximum speed."""
        # Convert DataFrame to CSV buffer
        buffer = io.StringIO()
        df.to_csv(buffer, index=False, header=False, na_rep='\\N')
        buffer.seek(0)

        target_engine = create_engine(self.target_url)
        with target_engine.raw_connection() as conn:
            with conn.cursor() as cursor:
                # Create table if not exists
                self._ensure_table_exists(cursor, target_table, df)

                # Use COPY FROM
                cursor.copy_expert(
                    f"""
                    COPY bronze.{target_table}
                    FROM STDIN WITH (
                        FORMAT CSV,
                        DELIMITER ',',
                        NULL '\\N'
                    )
                    """,
                    buffer
                )
                conn.commit()

        return len(df)
```

### **2. Incremental Extraction with Timestamps**

```python
class IncrementalPostgreSQLExtractor(PostgreSQLExtractor):
    """Extract incrementally based on timestamp columns."""

    def extract_incremental(
        self,
        table_name: str,
        timestamp_column: str = 'updated_at',
        schema: str = 'public',
        lookback_hours: int = 1
    ) -> int:
        """Extract only new/modified records."""

        # Get last extraction checkpoint
        checkpoint = self._get_checkpoint(schema, table_name)

        # Apply lookback for safety
        from datetime import timedelta
        if checkpoint:
            checkpoint = checkpoint - timedelta(hours=lookback_hours)

        logger.info(f"Incremental extraction from {checkpoint or 'beginning'}")

        # Build incremental query
        query = f"""
            SELECT *
            FROM "{schema}"."{table_name}"
            WHERE "{timestamp_column}" > %s
            ORDER BY "{timestamp_column}"
        """

        # Extract new records
        with self.get_connection() as conn:
            df = pd.read_sql(
                query,
                conn,
                params=[checkpoint or '1900-01-01'],
                parse_dates=[timestamp_column]
            )

        if df.empty:
            logger.info("No new records to extract")
            return 0

        # Add Bronze metadata
        df['_bronze_loaded_at'] = datetime.now()
        df['_bronze_extraction_type'] = 'incremental'
        df['_bronze_checkpoint'] = checkpoint

        # Write to Bronze (append mode)
        rows = self._write_incremental(df, table_name)

        # Update checkpoint
        self._save_checkpoint(
            schema, table_name,
            df[timestamp_column].max()
        )

        return rows

    def _get_checkpoint(self, schema: str, table_name: str) -> Optional[datetime]:
        """Get last successful extraction checkpoint."""
        target_engine = create_engine(self.target_url)

        query = """
            SELECT MAX(checkpoint_value)
            FROM bronze.extraction_checkpoints
            WHERE source_schema = %s
            AND source_table = %s
            AND status = 'success'
        """

        with target_engine.connect() as conn:
            result = conn.execute(query, (schema, table_name)).scalar()
            return result

    def _save_checkpoint(self, schema: str, table_name: str, checkpoint: datetime):
        """Save extraction checkpoint."""
        target_engine = create_engine(self.target_url)

        with target_engine.connect() as conn:
            conn.execute("""
                INSERT INTO bronze.extraction_checkpoints
                (source_schema, source_table, checkpoint_value, status, created_at)
                VALUES (%s, %s, %s, 'success', CURRENT_TIMESTAMP)
            """, (schema, table_name, checkpoint))
```

### **3. Partitioned Table Extraction**

```python
class PartitionedPostgreSQLExtractor(PostgreSQLExtractor):
    """Extract partitioned tables efficiently."""

    def extract_partitioned_table(
        self,
        parent_table: str,
        partition_column: str = 'created_date',
        schema: str = 'public',
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> int:
        """Extract partitioned table by processing each partition separately."""

        # Get partition information
        partitions = self._get_partitions(schema, parent_table)

        total_rows = 0
        for partition in partitions:
            # Check if partition is in date range
            if not self._partition_in_range(partition, start_date, end_date):
                continue

            logger.info(f"Extracting partition: {partition['name']}")

            # Extract partition directly for efficiency
            rows = self._extract_partition(
                schema,
                partition['name'],
                parent_table
            )

            total_rows += rows

        logger.info(f"Total rows extracted from partitioned table: {total_rows}")
        return total_rows

    def _get_partitions(self, schema: str, parent_table: str) -> list:
        """Get list of partitions for a table."""
        query = """
            SELECT
                child.relname as partition_name,
                pg_get_expr(child.relpartbound, child.oid) as partition_expression
            FROM pg_inherits
            JOIN pg_class parent ON pg_inherits.inhparent = parent.oid
            JOIN pg_class child ON pg_inherits.inhrelid = child.oid
            JOIN pg_namespace n ON parent.relnamespace = n.oid
            WHERE n.nspname = %s
            AND parent.relname = %s
            ORDER BY child.relname
        """

        with self.get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(query, (schema, parent_table))
                return [
                    {'name': row[0], 'expression': row[1]}
                    for row in cursor.fetchall()
                ]

    def _extract_partition(
        self,
        schema: str,
        partition_name: str,
        parent_table: str
    ) -> int:
        """Extract a single partition."""

        # Direct partition access is faster
        query = f"""
            SELECT *
            FROM "{schema}"."{partition_name}"
        """

        with self.get_connection() as conn:
            df = pd.read_sql(query, conn)

        if df.empty:
            return 0

        # Add metadata
        df['_bronze_loaded_at'] = datetime.now()
        df['_bronze_partition'] = partition_name
        df['_bronze_parent_table'] = parent_table

        # Write to Bronze
        return self._write_to_bronze(df, parent_table)
```

### **4. JSONB and Array Handling**

```python
class JSONBPostgreSQLExtractor(PostgreSQLExtractor):
    """Handle PostgreSQL JSONB and array types."""

    def extract_with_json(
        self,
        table_name: str,
        jsonb_columns: list,
        array_columns: list = None,
        schema: str = 'public'
    ) -> int:
        """Extract tables with JSONB and array columns."""

        # Build query with JSON extraction
        columns = self._build_json_column_list(
            schema, table_name, jsonb_columns, array_columns
        )

        query = f"""
            SELECT {columns}
            FROM "{schema}"."{table_name}"
        """

        with self.get_connection() as conn:
            # Use custom converter for JSONB
            df = pd.read_sql(
                query,
                conn,
                dtype_backend='numpy_nullable'
            )

        # Process JSONB columns
        for col in jsonb_columns:
            if col in df.columns:
                df[f'{col}_parsed'] = df[col].apply(self._parse_json_safe)

        # Process array columns
        if array_columns:
            for col in array_columns:
                if col in df.columns:
                    df[f'{col}_expanded'] = df[col].apply(self._expand_array)

        # Add Bronze metadata
        df['_bronze_loaded_at'] = datetime.now()

        # Write to Bronze
        return self._write_to_bronze(df, table_name)

    def _build_json_column_list(
        self,
        schema: str,
        table_name: str,
        jsonb_columns: list,
        array_columns: list
    ) -> str:
        """Build column list with JSON operations."""
        columns = []

        # Get all columns
        with self.get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT column_name, data_type
                    FROM information_schema.columns
                    WHERE table_schema = %s
                    AND table_name = %s
                """, (schema, table_name))

                for col_name, col_type in cursor.fetchall():
                    if col_name in jsonb_columns:
                        # Cast JSONB to text for DataFrame compatibility
                        columns.append(f'"{col_name}"::text as "{col_name}"')
                    elif col_name in (array_columns or []):
                        # Convert array to JSON for easier parsing
                        columns.append(f'array_to_json("{col_name}") as "{col_name}"')
                    else:
                        columns.append(f'"{col_name}"')

        return ', '.join(columns)

    def _parse_json_safe(self, json_str: str) -> dict:
        """Safely parse JSON string."""
        if pd.isna(json_str):
            return None
        try:
            import json
            return json.loads(json_str)
        except (json.JSONDecodeError, TypeError):
            return {'_error': 'parse_failed', '_raw': str(json_str)}

    def _expand_array(self, array_data: Any) -> list:
        """Expand PostgreSQL array."""
        if pd.isna(array_data):
            return []
        if isinstance(array_data, str):
            import json
            try:
                return json.loads(array_data)
            except:
                return []
        return list(array_data)
```

### **5. Logical Replication / CDC**

```python
class CDCPostgreSQLExtractor(PostgreSQLExtractor):
    """Extract changes using logical replication."""

    def setup_replication_slot(self, slot_name: str = 'datakit_slot'):
        """Set up logical replication slot."""
        with self.get_connection() as conn:
            with conn.cursor() as cursor:
                # Create replication slot if not exists
                cursor.execute("""
                    SELECT * FROM pg_replication_slots
                    WHERE slot_name = %s
                """, (slot_name,))

                if not cursor.fetchone():
                    cursor.execute("""
                        SELECT pg_create_logical_replication_slot(%s, 'pgoutput')
                    """, (slot_name,))

                    logger.info(f"Created replication slot: {slot_name}")

    def extract_changes(
        self,
        slot_name: str = 'datakit_slot',
        publication_name: str = 'datakit_publication'
    ) -> int:
        """Extract changes from replication slot."""
        import psycopg2.extras

        changes = []

        # Create replication connection
        repl_conn = psycopg2.connect(
            **self.source_config,
            connection_factory=psycopg2.extras.LogicalReplicationConnection
        )

        cursor = repl_conn.cursor()

        try:
            # Start replication
            cursor.start_replication(
                slot_name=slot_name,
                options={'publication_names': publication_name}
            )

            # Consume changes
            cursor.consume_stream(
                lambda msg: self._process_replication_message(msg, changes),
                timeout=10.0
            )

        finally:
            cursor.close()
            repl_conn.close()

        # Write changes to Bronze
        if changes:
            df = pd.DataFrame(changes)
            return self._write_cdc_to_bronze(df)

        return 0

    def _process_replication_message(self, msg, changes: list):
        """Process a replication message."""
        import json

        # Parse the message
        data = json.loads(msg.payload)

        change = {
            'lsn': msg.data_start,
            'timestamp': datetime.now(),
            'operation': data.get('action'),  # INSERT, UPDATE, DELETE
            'schema': data.get('schema'),
            'table': data.get('table'),
            'data': data.get('columns'),
            '_bronze_loaded_at': datetime.now()
        }

        changes.append(change)

        # Acknowledge message
        msg.cursor.send_feedback(flush_lsn=msg.data_start)

    def _write_cdc_to_bronze(self, df: pd.DataFrame) -> int:
        """Write CDC changes to Bronze layer."""
        target_engine = create_engine(self.target_url)

        # Write to CDC table
        df.to_sql(
            name='cdc_changes',
            con=target_engine,
            schema='bronze',
            if_exists='append',
            index=False
        )

        return len(df)
```

## ðŸ”§ Configuration Options

### **Connection Configuration**

```python
# .env file
SOURCE_DATABASE_URL=postgresql://user:password@host:5432/database

# Advanced connection options
SOURCE_PG_OPTIONS='-c statement_timeout=300000 -c lock_timeout=10000'
SOURCE_PG_SSLMODE=require
SOURCE_PG_SSLCERT=/certs/client.crt
SOURCE_PG_SSLKEY=/certs/client.key
SOURCE_PG_SSLROOTCERT=/certs/ca.crt
SOURCE_PG_POOL_SIZE=10
SOURCE_PG_POOL_MAX_OVERFLOW=20
SOURCE_PG_POOL_TIMEOUT=30
```

### **Performance Tuning**

```python
class OptimizedPostgreSQLExtractor(PostgreSQLExtractor):
    """PostgreSQL extractor with performance optimizations."""

    def __init__(self):
        super().__init__()
        self.optimization_settings = {
            'work_mem': '256MB',
            'maintenance_work_mem': '512MB',
            'effective_cache_size': '4GB',
            'random_page_cost': 1.1,
            'effective_io_concurrency': 200,
            'max_parallel_workers_per_gather': 4
        }

    def extract_optimized(self, table_name: str) -> int:
        """Extract with query optimization."""
        with self.get_connection() as conn:
            with conn.cursor() as cursor:
                # Set session-level optimization
                for setting, value in self.optimization_settings.items():
                    cursor.execute(f"SET {setting} = %s", (value,))

                # Analyze table for fresh statistics
                cursor.execute(f'ANALYZE "{table_name}"')

                # Use optimized extraction
                return self.extract_table_fast('public', table_name)
```

## ðŸ§ª Testing PostgreSQL Datakits

### **Unit Tests**

```python
# tests/test_postgres_extractor.py
import pytest
from unittest.mock import Mock, patch, MagicMock
import pandas as pd
from datakit_postgres.extractor import PostgreSQLExtractor

class TestPostgreSQLExtractor:

    @pytest.fixture
    def extractor(self):
        with patch.dict('os.environ', {
            'SOURCE_DATABASE_URL': 'postgresql://test:test@localhost:5432/test',
            'WAREHOUSE_DATABASE_URL': 'postgresql://test:test@localhost:5432/warehouse'
        }):
            with patch('psycopg2.pool.ThreadedConnectionPool'):
                return PostgreSQLExtractor()

    def test_extraction_query_building(self, extractor):
        """Test query building logic."""
        query = extractor._build_extraction_query(
            schema='public',
            table_name='users',
            columns=['id', 'name', 'email'],
            where_clause='active = true'
        )

        assert query == 'SELECT "id", "name", "email" FROM "public"."users" WHERE active = true'

    @patch('pandas.read_sql')
    def test_extraction_with_chunking(self, mock_read_sql, extractor):
        """Test chunked extraction."""
        # Mock data
        mock_df = pd.DataFrame({
            'id': [1, 2, 3],
            'name': ['Alice', 'Bob', 'Charlie']
        })
        mock_read_sql.return_value = mock_df

        with patch.object(extractor, 'get_connection'):
            df = extractor._extract_with_chunking("SELECT * FROM users")

        assert len(df) == 3
        assert list(df.columns) == ['id', 'name']

    @patch('psycopg2.connect')
    def test_copy_protocol_extraction(self, mock_connect):
        """Test COPY protocol extraction."""
        mock_cursor = MagicMock()
        mock_connect.return_value.__enter__.return_value.cursor.return_value = mock_cursor

        extractor = FastPostgreSQLExtractor()
        extractor.extract_table_fast('public', 'users')

        # Verify COPY command was used
        mock_cursor.copy_expert.assert_called_once()
        call_args = mock_cursor.copy_expert.call_args[0][0]
        assert 'COPY' in call_args
        assert 'TO STDOUT' in call_args
```

### **Integration Tests**

```python
# tests/integration/test_postgres_integration.py
import pytest
import psycopg2
import pandas as pd
from testcontainers.postgres import PostgresContainer
from datakit_postgres.extractor import PostgreSQLExtractor

@pytest.mark.integration
class TestPostgreSQLIntegration:

    @pytest.fixture(scope='class')
    def postgres_container(self):
        """Start PostgreSQL container for testing."""
        with PostgresContainer('postgres:14') as postgres:
            yield postgres

    @pytest.fixture
    def setup_test_data(self, postgres_container):
        """Set up test data in PostgreSQL."""
        conn = psycopg2.connect(
            host=postgres_container.get_container_host_ip(),
            port=postgres_container.get_exposed_port(5432),
            database=postgres_container.POSTGRES_DB,
            user=postgres_container.POSTGRES_USER,
            password=postgres_container.POSTGRES_PASSWORD
        )

        with conn.cursor() as cursor:
            # Create test table
            cursor.execute("""
                CREATE TABLE test_users (
                    id SERIAL PRIMARY KEY,
                    name VARCHAR(100),
                    email VARCHAR(100),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Insert test data
            cursor.execute("""
                INSERT INTO test_users (name, email)
                VALUES
                    ('Alice', 'alice@example.com'),
                    ('Bob', 'bob@example.com'),
                    ('Charlie', 'charlie@example.com')
            """)

            conn.commit()

        yield conn
        conn.close()

    def test_full_extraction(self, postgres_container, setup_test_data):
        """Test full table extraction."""
        import os
        os.environ['SOURCE_DATABASE_URL'] = postgres_container.get_connection_url()
        os.environ['WAREHOUSE_DATABASE_URL'] = postgres_container.get_connection_url()

        extractor = PostgreSQLExtractor()
        rows = extractor.extract_table('test_users')

        assert rows == 3

    def test_incremental_extraction(self, postgres_container, setup_test_data):
        """Test incremental extraction."""
        # Implementation of incremental test
        pass
```

## ðŸš¨ Common Issues and Solutions

### **Issue: Connection Pool Exhaustion**

```python
class RobustPostgreSQLExtractor(PostgreSQLExtractor):
    """Handle connection pool issues."""

    def __init__(self):
        super().__init__()
        self.max_retries = 3
        self.retry_delay = 1

    @contextmanager
    def get_connection_with_retry(self):
        """Get connection with retry logic."""
        import time

        for attempt in range(self.max_retries):
            try:
                conn = self.connection_pool.getconn()
                yield conn
                self.connection_pool.putconn(conn)
                return
            except psycopg2.pool.PoolError as e:
                if attempt < self.max_retries - 1:
                    logger.warning(f"Connection pool exhausted, retry {attempt + 1}")
                    time.sleep(self.retry_delay * (attempt + 1))
                else:
                    raise
            except Exception:
                self.connection_pool.putconn(conn, close=True)
                raise
```

### **Issue: Large Object Handling**

```python
def extract_with_large_objects(self, table_name: str):
    """Handle tables with large objects (LOBs)."""
    with self.get_connection() as conn:
        # Set fetch size for large objects
        with conn.cursor() as cursor:
            cursor.arraysize = 100  # Fetch 100 rows at a time

            # Use lo_export for large objects
            cursor.execute(f"""
                SELECT
                    id,
                    lo_get(large_object_oid) as file_content,
                    metadata
                FROM "{table_name}"
            """)

            # Process in batches
            while True:
                rows = cursor.fetchmany()
                if not rows:
                    break

                # Process large objects
                for row in rows:
                    self._process_large_object(row)
```

### **Issue: Long-Running Queries**

```python
def extract_with_timeout(self, table_name: str, timeout_seconds: int = 300):
    """Extract with query timeout."""
    with self.get_connection() as conn:
        with conn.cursor() as cursor:
            # Set statement timeout
            cursor.execute('SET statement_timeout = %s', (timeout_seconds * 1000,))

            try:
                # Execute extraction
                cursor.execute(f'SELECT * FROM "{table_name}"')
                return cursor.fetchall()
            except psycopg2.errors.QueryCanceled:
                logger.error(f"Query timeout after {timeout_seconds} seconds")
                raise
```

## ðŸ“Š Performance Benchmarks

| Extraction Method | 1M Rows | 10M Rows | 100M Rows | Memory Usage |
|------------------|---------|----------|-----------|--------------|
| Basic read_sql | 15s | 3m | 45m | 2GB |
| Chunked read_sql | 18s | 3.5m | 50m | 500MB |
| COPY TO/FROM | 5s | 45s | 8m | 300MB |
| Server-side cursor | 20s | 4m | 55m | 100MB |
| Parallel extraction | 8s | 1.5m | 20m | 1GB |

## ðŸ”— Related Documentation

- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [psycopg2 Documentation](https://www.psycopg.org/docs/)
- [SQLAlchemy PostgreSQL Dialect](https://docs.sqlalchemy.org/en/14/dialects/postgresql.html)
- [Datakit Architecture](../architecture.md)
- [Testing Strategies](../testing.md)

---

*[â† Back to Creating Datakits](../../creating-datakits.md) | [Next: SQL Server Datakits â†’](sqlserver.md)*